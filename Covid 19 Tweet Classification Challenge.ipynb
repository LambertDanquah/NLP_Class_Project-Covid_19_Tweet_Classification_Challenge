{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BynU6Cr0bQtY","executionInfo":{"status":"ok","timestamp":1680587461066,"user_tz":0,"elapsed":4354470,"user":{"displayName":"Lambert Danquah","userId":"10168157469660409211"}},"outputId":"1b4d71d1-6822-49f5-ed6a-1da62b3d57c3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"output_type":"stream","name":"stdout","text":["529/529 [==============================] - 257s 394ms/step - loss: 0.5598 - accuracy: 0.7526\n","Epoch 2/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.3846 - accuracy: 0.8774\n","Epoch 3/20\n","529/529 [==============================] - 207s 392ms/step - loss: 0.3430 - accuracy: 0.8882\n","Epoch 4/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.3152 - accuracy: 0.9033\n","Epoch 5/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.3043 - accuracy: 0.9079\n","Epoch 6/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.2932 - accuracy: 0.9122\n","Epoch 7/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.2896 - accuracy: 0.9109\n","Epoch 8/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.2807 - accuracy: 0.9187\n","Epoch 9/20\n","529/529 [==============================] - 208s 392ms/step - loss: 0.2783 - accuracy: 0.9170\n","Epoch 10/20\n","529/529 [==============================] - 207s 392ms/step - loss: 0.2742 - accuracy: 0.9181\n","Epoch 11/20\n","529/529 [==============================] - 208s 393ms/step - loss: 0.2663 - accuracy: 0.9249\n","Epoch 12/20\n","529/529 [==============================] - 208s 392ms/step - loss: 0.2613 - accuracy: 0.9242\n","Epoch 13/20\n","529/529 [==============================] - 207s 392ms/step - loss: 0.2589 - accuracy: 0.9253\n","Epoch 14/20\n","529/529 [==============================] - 207s 392ms/step - loss: 0.2578 - accuracy: 0.9279\n","Epoch 15/20\n","529/529 [==============================] - 207s 392ms/step - loss: 0.2540 - accuracy: 0.9293\n","Epoch 16/20\n","529/529 [==============================] - 208s 392ms/step - loss: 0.2542 - accuracy: 0.9274\n","Epoch 17/20\n","529/529 [==============================] - 207s 392ms/step - loss: 0.2495 - accuracy: 0.9308\n","Epoch 18/20\n","529/529 [==============================] - 208s 394ms/step - loss: 0.2434 - accuracy: 0.9334\n","Epoch 19/20\n","529/529 [==============================] - 208s 394ms/step - loss: 0.2428 - accuracy: 0.9313\n","Epoch 20/20\n","529/529 [==============================] - 208s 394ms/step - loss: 0.2439 - accuracy: 0.9329\n"]}],"source":["\n","#!pip install -U tensorflow==2.5.0\n","#!pip install numpy==1.19.2\n","#import tensorflow_hub as hub\n","#import tensorflow_text as text\n","#!pip install pyenchant\n","#!pip install transformers\n","\n","import tensorflow as tf\n","import pandas as pd, numpy as np\n","import warnings, string\n","import spacy, re, nltk\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","import enchant\n","import transformers\n","from transformers import AutoTokenizer, TFBertModel\n","from transformers.models.bert.modeling_flax_bert import BertConfig\n","\n","#import keras\n","#from keras import initializers\n","#from keras.utils import io_utils\n","#from tensorflow.python.util.tf_export import keras_export\n","from tensorflow.keras.optimizers.legacy import Adam\n","#from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy, BinaryAccuracy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Input, Dense\n","\n","#from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","warnings.filterwarnings('ignore', category=DeprecationWarning)\n","warnings.filterwarnings('ignore', category=FutureWarning)\n","my_spacy= spacy.load('en_core_web_sm')\n","\n","tweet = pd.read_csv('/content/drive/MyDrive/AAI_project_1/Covid-19 Tweet/Train 2.csv')\n","test_tweet_df = pd.read_csv('/content/drive/MyDrive/AAI_project_1/Covid-19 Tweet/Test 2.csv')\n","submission = pd.read_csv('/content/drive/MyDrive/AAI_project_1/Covid-19 Tweet/SampleSubmission 2.csv')\n","\n","Y = tweet['target']\n","\n","\n","\n","#column_features = ['ID', 'target']\n","#tweet = tweet.drop(column_features, axis = 1)\n","\n","\n","\n","#\n","\n","#REMOVE DIGITS\n","tweet['text'] = tweet['text'].str.replace(r'\\d+', '')\n","\n","#check for mispelled words and correct them\n","english_dict = enchant.Dict('en_US')\n","\n","def correct_spelling(text):\n","    words = text.split()\n","    corrected_words = []\n","    for word in words:\n","        if not english_dict.check(word):\n","            suggestions = english_dict.suggest(word)\n","            if suggestions:\n","                corrected_words.append(suggestions[0])\n","            else:\n","                corrected_words.append(word)\n","        else:\n","            corrected_words.append(word)\n","    return ' '.join(corrected_words)\n","\n","tweet['text'] = tweet['text'].apply(correct_spelling)\n","            \n","                \n","\n","#TOKENIZE TWEETS\n","def tokenize(txt):\n","    tokens = re.split('\\W+', txt)\n","    return tokens\n","           \n","tweet['text'] = tweet['text'].apply(lambda x: tokenize(x.lower()))\n","#print(tweet['text'])\n","\n","#REMOVE STOPWORDS\n","my_spacy_stopwords = my_spacy.Defaults.stop_words\n","\n","def remove_stopwords (tokenized_tweet):\n","    updated_tweet = [message for message in tokenized_tweet if message not in my_spacy_stopwords]\n","    return updated_tweet\n","\n","tweet['text'] = tweet['text'].apply(lambda x: remove_stopwords(x))\n","\n","\n","                        \n","#Convert list of tokens back to strings\n","tweet['text'] = [' '.join(t) for t in tweet['text']]\n","\n","\n","#LEMMATIZATION\n","tweet['text'] = tweet['text'].apply(lambda x: \" \".join([y.lemma_ for y in my_spacy(x)]))\n","\n","\n","X = tweet['text']\n","\n","#model = AutoModelForMaskedLM.from_pretrained(\"vinai/bertweet-covid19-base-uncased\")\n","#model = TFBertForMaskedLM.from_pretrained(\"vinai/bertweet-covid19-base-uncased\")\n","Btokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n","\n","bert = TFBertModel.from_pretrained('bert-large-uncased')\n","config = BertConfig()\n","config.vocab_size = Btokenizer.vocab_size\n","#Bmodel = TFBertModel.from_pretrained\n","#Btokenizer\n","\n","#tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-covid19-base-uncased\")\n","#Bmodel = TFBertModel.from_pretrained(\"vinai/bertweet-covid19-base-uncased\")\n","\n","#Convert text in dataframe into BERT input format\n","#print('max length of tweets', max([len(x.split()) for x in tweet.text]))\n","#tf.keras.utils.warmstart_embedding_matrix(\n","#    base_vocabulary, new_vocabulary, base_embeddings,\n","#    new_embeddings_initializer='uniform')\n","\n","X_Train = Btokenizer(text = X.tolist(), add_special_tokens = True, max_length = 45,\n","                     truncation = True, padding = True, return_tensors = 'tf',\n","                     return_token_type_ids = False, return_attention_mask = True, verbose = True)\n","\n","Y_train = tweet['target'].values\n","\n","max_len = 45\n","input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n","input_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n","\n","embeddings = bert(input_ids, attention_mask=input_mask)[1]\n","out =tf.keras.layers.Dropout(0.1)(embeddings)\n","\n","out = Dense(128, activation = 'relu')(out)\n","out = tf.keras.layers.Dropout(0.1)(out)\n","out = Dense(32, activation = 'sigmoid')(out)\n","\n","y = Dense(1, activation = 'sigmoid')(out)\n","\n","model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n","model.layers[2].trainable = True\n","\n","optimizer = Adam(learning_rate=6e-06, epsilon=1e-08, decay=0.01, clipnorm=1.0)\n","\n","#Set loss and metrics\n","loss = BinaryCrossentropy(from_logits =  True)\n","metric = BinaryAccuracy('accuracy')\n","\n","#compile the model\n","model.compile(optimizer = optimizer, loss = loss, metrics = metric)\n","\n","#train the model\n","train_model = model.fit(x = {'input_ids': X_Train['input_ids'], 'attention_mask': X_Train['attention_mask']}, \n","                        y = Y_train, epochs = 20, batch_size = 10)\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZnR9Xj0CSi5u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680588878019,"user_tz":0,"elapsed":99716,"user":{"displayName":"Lambert Danquah","userId":"10168157469660409211"}},"outputId":"05b71a89-db2c-4b42-d30f-4ae7cba74f0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["62/62 [==============================] - 26s 330ms/step\n","Done\n"]}],"source":["\n","#REMOVE DIGITS\n","test_tweet_df['text'] = test_tweet_df['text'].str.replace(r'\\d+', '')\n","\n","#check for mispelled words and correct them\n","english_dict = enchant.Dict('en_US')\n","\n","def correct_spelling(text):\n","    words = text.split()\n","    corrected_words = []\n","    for word in words:\n","        if not english_dict.check(word):\n","            suggestions = english_dict.suggest(word)\n","            if suggestions:\n","                corrected_words.append(suggestions[0])\n","            else:\n","                corrected_words.append(word)\n","        else:\n","            corrected_words.append(word)\n","    return ' '.join(corrected_words)\n","\n","test_tweet_df['text'] = test_tweet_df['text'].apply(correct_spelling)\n","            \n","                \n","\n","#TOKENIZE TWEETS\n","def tokenize(txt):\n","    tokens = re.split('\\W+', txt)\n","    return tokens\n","           \n","test_tweet_df['text'] = test_tweet_df['text'].apply(lambda x: tokenize(x.lower()))\n","#print(tweet['text'])\n","\n","#REMOVE STOPWORDS\n","my_spacy_stopwords = my_spacy.Defaults.stop_words\n","\n","def remove_stopwords (tokenized_tweet):\n","    updated_tweet = [message for message in tokenized_tweet if message not in my_spacy_stopwords]\n","    return updated_tweet\n","\n","test_tweet_df['text'] = test_tweet_df['text'].apply(lambda x: remove_stopwords(x))\n","\n","\n","                        \n","#Convert list of tokens back to strings\n","test_tweet_df['text'] = [' '.join(t) for t in test_tweet_df['text']]\n","\n","\n","#LEMMATIZATION\n","test_tweet_df['text'] = test_tweet_df['text'].apply(lambda x: \" \".join([y.lemma_ for y in my_spacy(x)]))\n","\n","\n","X_new = test_tweet_df['text']\n","\n","#Predict values from new dataset\n","X_Test = Btokenizer(text = X_new .tolist(), add_special_tokens = True, max_length = 45,\n","                     truncation = True, padding = True, return_tensors = 'tf',\n","                     return_token_type_ids = False, return_attention_mask = True, verbose = True)\n","\n","predicted = model.predict( {'input_ids': X_Test['input_ids'], 'attention_mask': X_Test['attention_mask']})\n","#submission['ID'] = test_tweet_df.id\n","submission['target'] = predicted\n","submission.to_csv('/content/drive/MyDrive/AAI_project_1/Covid-19 Tweet/TweetSubmit.csv', index = False)\n","submission.to_csv('Submit.csv', index = False)\n","print('Done')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1CcZoHH5eN3j4SW4ubLMiNRXH1zBgnIUo","authorship_tag":"ABX9TyMlBj2pHShN3nt4G27gqR6U"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}